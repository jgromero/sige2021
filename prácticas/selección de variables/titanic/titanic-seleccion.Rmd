---
title: "Reducción de datos con conjunto de datos Titanic"
date: "17 de marzo de 2021"
output:
  html_document:
      code_folding: "show"
      toc: true
      toc_depth: 2
      toc_float: true
      df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(pROC)
library(funModeling)
library(rpart.plot)
library(Hmisc)
library(corrplot)

set.seed(0)
```

Reducción de datos con el dataset [titanic](https://www.kaggle.com/c/titanic/).

**En este cuaderno ampliaremos el código desarrollado en [titanic.Rmd](https://github.com/jgromero/sige2021/blob/main/teor%C3%ADa/tema%203/titanic/titanic.Rmd) para seleccionar automáticamente las variables que se utilizarán para construir el modelo de predicción.**

Para ello, nos basaremos en la premisa de que diversos algoritmos de [<tt>caret</tt>](http://topepo.github.io/caret/) [ya incluyen procedimientos para selección de variables](https://topepo.github.io/caret/feature-selection-overview.html#builtin). 

Para un ejemplo detallado basado en uso de correlaciones entre variables, ver [este análisis con el problema Lending Club](https://github.com/jgromero/sige2021/blob/main/pr%C3%A1cticas/selecci%C3%B3n%20de%20variables/lending%20club/lending_club_seleccion.Rmd).

# Lectura y preprocesamiento de datos
Comenzamos leyendo el fichero de datos:
```{r lectura}
library(tidyverse)
data_raw <- read_csv('train.csv')
head(data_raw)
```

A continuación, preprocesamos los datos para quedarnos con las variables que vamos a utilizar y recodificar la variable objetivo `Survived`. Las filas con valores NA se excluyen del proceso:
```{r preprocesamiento}
data <-
  data_raw %>%
  mutate(Survived = as.factor(ifelse(Survived == 1, 'Yes', 'No'))) %>%
  mutate(Pclass = as.factor(Pclass)) %>%
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
  na.exclude()
```

#2. Creación de modelo predictivo
Una vez están listos los datos, podemos crear un modelo predictivo que evalúa la importancia de las variables, como por ejemplo [<tt>rpart</tt>](https://cran.r-project.org/web/packages/rpart/).

### Entrenamiento
Creamos el modelo predictivo:
```{r entrenamiento-rpart}
# Parámetros
rpartCtrl <- trainControl(verboseIter = F, classProbs = TRUE, summaryFunction = twoClassSummary)
rpartParametersGrid <- expand.grid(.cp = c(0.01, 0.05))

# Conjuntos de entrenamiento y validación
trainIndex <- createDataPartition(data$Survived, p = .8, list = FALSE, times = 1)
train <- data[trainIndex, ] 

# Entrenamiento del modelo
rpartModel <- train(Survived ~ ., 
                    data = train, 
                    method = "rpart", 
                    metric = "ROC", 
                    trControl = rpartCtrl, 
                    tuneGrid = rpartParametersGrid)

# Visualización del modelo
rpart.plot(rpartModel$finalModel)
```

### Validación
Obtenemos resultados con el conjunto de validación:
```{r validacion-rpart}
# Predicciones con clases
val        <- data[-trainIndex, ]
prediction <- predict(rpartModel, val, type = "raw") 

# Predicciones con probabilidades
predictionValidationProb <- predict(rpartModel, val, type = "prob")
```

Y calculamos las métricas de calidad del clasificador (matriz de confusión y curva ROC):
```{r validacion-metricas}
cm_train <- confusionMatrix(prediction, val[["Survived"]])
cm_train

auc <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
roc_validation <- plot.roc(auc, 
                           ylim=c(0,1), 
                           type = "S" , 
                           print.thres = TRUE, 
                           main=paste('Validation AUC:', round(auc$auc[[1]], 2)))
```

### Otros modelos de predicción
Otro modelo de predicción que calcula importancia de variables es [<tt>rf</tt>](https://cran.r-project.org/web/packages/randomForest/) (Random Forest):
```{r entrenamiento-rf}
rfModel <- train(Survived ~ ., data = train, method = "rf", metric = "ROC", trControl = rpartCtrl)
predictionValidationProb <- predict(rfModel, val, type = "prob")
auc <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
roc_validation <- plot.roc(auc, 
                           ylim=c(0,1), 
                           type = "S" , 
                           print.thres = TRUE, 
                           main=paste('Validation AUC:', round(auc$auc[[1]], 2)))
```

# Importancia de las variables con modelo de predicción
Una vez que los modelos han sido entrenados, podemos estudiar la importancia que cada otorga a las variabales utilizando [<tt>varImp</tt>](https://topepo.github.io/caret/variable-importance.html) en [<tt>caret</tt>](http://topepo.github.io/caret/):

```{r}
varImp(rpartModel)
varImp(rfModel)
```

El ranking obtenido puede servir para seleccionar las variables que se utilizarán para abordar el problema; por ejemplo aquellas con valor `Overall` por encima de 25. Esta selección se puede automatizar parcialmente, ya que tanto <tt>rpart</tt> como <tt>rf</tt> usan variables 'dummy' (por ejemplo, `Sexmale`) que no están en el dataset original.

```{r}
important_vars <- varImp(rfModel)$importance %>%
  rownames_to_column() %>%
  filter(Overall > 25) %>%
  select(rowname)
```

El orden de importancia se corresponden aproximadamente con el valor absoluto de correlación entre cada variable y la variable objetivo:
```{r}
# correlation_table(data, target='Survived')
data_num <-
  data %>%
  mutate_if(is.character, as.factor) %>%
  mutate_if(is.factor, as.numeric)
cor(data_num)
# correlation_table(data_num, target='Survived')
```

Visualmente, podemos verlo como tabla de correlación o como mapa de calor:
```{r}
rcorr(as.matrix(data_num))
corrplot(cor(data_num), type = "upper", diag = F, order = "hclust", tl.col = "black", tl.srt = 45)
heatmap(x = cor(data_num), symm = TRUE)
```

# Importancia de las variables sin modelo de predicción
También se puede conocer la importancia de las variables sin crear explícitamente un modelo de predicción, sino simplemente calculando los mismos parámetros que se utilizan para aprender los modelos. Por ejemplo, los modelos de árboles utilizan la ganancia de información o la entropía para decidir sobre qué variables ramificar. Con [var_rank_info](https://www.rdocumentation.org/packages/funModeling/versions/1.9.3/topics/var_rank_info) podemos estimar varios estos valores directamente:

* en: entropy measured in bits
* mi: mutual information
* ig: information gain
* gr: gain ratio

```{r}
var_rank_info(data, "Survived")
```

# Entrenamiento del modelo con selección de variables
Una vez determinadas las variables importantes, podemos entrenar modelos seleccionando solamente estas variables importantes:
```{r}
data_reduced <-
  data %>%
  select(Survived, Sex, Fare, Age, Pclass, SibSp)
head(data_reduced)

train <- data_reduced[trainIndex, ] 
val   <- data_reduced[-trainIndex, ]

# rpart
rpartModel_2 <- train(Survived ~ ., data = train, method = "rpart", metric = "ROC", trControl = rpartCtrl, tuneGrid = rpartParametersGrid)
predictionValidationProb <- predict(rpartModel_2, val, type = "prob")
auc_rpart <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
auc_rpart
roc_rpart <- plot.roc(auc_rpart, ylim=c(0,1), type = "S" , print.thres = T, main=paste('Validation AUC:', round(auc_rpart$auc[[1]], 2)))

# rf
rfModel_2 <- train(Survived ~ ., data = train, method = "rf", metric = "ROC", trControl = rpartCtrl)
predictionValidationProb <- predict(rfModel_2, val, type = "prob")
auc_rf <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
auc_rf
roc_rf <- plot.roc(auc_rf, ylim=c(0,1), type = "S" , print.thres = T, main=paste('Validation AUC:', round(auc_rf$auc[[1]], 2)))

# comparacion
roc.test(roc_rf, roc_rpart)

plot.roc(auc_rpart, type = "S", col="#1c61b6")
lines.roc(auc_rf, type = "S", col="#008600")
```
